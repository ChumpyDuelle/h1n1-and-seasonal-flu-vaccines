{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![vaccine](images/vaccine.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# H1N1 and Seasonal Flu Vaccines\n",
    "\n",
    "**Authors**: Ian Butler, Red the dog\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "A one-paragraph overview of the project, including the business problem, data, methods, results and recommendations.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Problem\n",
    "\n",
    "Summary of the business problem we are trying to solve, and the data questions that we plan to answer to solve them.\n",
    "\n",
    "***\n",
    "\n",
    "Questions to consider:\n",
    "\n",
    "* What are the business's pain points related to ths project?\n",
    "    * The business's pain points related to this project are these:\n",
    "        * **Primary**: can we predict whether people got H1N1 vaccines using data collected in the National 2009 H1N1 Flu Survey?\n",
    "        * **Secondary**: can we infer which variables of this survey are most causing a change in vaccination status for H1N1?\n",
    "        * **Tertiary**: can we provide actionable recommendations to improve the rate of innoculation against H1N1?\n",
    "* How did we pick the data analysis question(s) that we did?\n",
    "    * With respect to each of the above questions, we picked the questions as a result of the following thought process:\n",
    "        * Ultimately, what we would like to be able to do is increase the population's rate of innoculation against highly infectious respiratory diseases. In order to know what would have to be done to take action in this way, we have to understand what variables are most causing a change in vaccination status for individuals, as well as which of those variables we can actually control. Finally, we can confirm whether or not our understanding of these relationships can be used to accurately predict vaccination status for individuals.\n",
    "* Why are these questions important from a business perspective?\n",
    "    * These questions are important from a business perspective because they allow public health officials to make informed decisions about what actions they can take to attempt to improve the resiliency of the population against highly infectious respiratory diseases.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Understanding\n",
    "\n",
    "Describe the data being used for this project.\n",
    "\n",
    "***\n",
    "\n",
    "Questions to consider:\n",
    "\n",
    "* Where did the data come from?\n",
    "    * The data comes from the National 2009 H1N1 Flu Survey.\n",
    "* How do they relate to the data analysis questions?\n",
    "    * The National 2009 H1N1 Flu Survery relates to the data analysis questions in that this survey was specifically designed to develop an understanding of how peopleâ€™s backgrounds, opinions, and health behaviors are related to their personal vaccination patterns.\n",
    "* What do the data represent?\n",
    "    * Each entry in the data represents the survey responses of one individual.\n",
    "* Who is in the sample?\n",
    "    * The sample is comprised of individuals from various psychographics and demographics across the United States.\n",
    "* What variable are includeed?\n",
    "    * The variables included are as follows:\n",
    "    * For all binary variables: 0 = No; 1 = Yes.\n",
    "        * **respondent_id**\n",
    "            * a unique and random identifier\n",
    "        * **h1n1_concern** - Level of concern about the H1N1 flu.\n",
    "            * 0 = Not at all concerned;\n",
    "            * 1 = Not very concerned;\n",
    "            * 2 = Somewhat concerned;\n",
    "            * 3 = Very concerned.\n",
    "        * **h1n1_knowledge** - Level of knowledge about the H1N1 flu.\n",
    "            * 0 = No knowledge;\n",
    "            * 1 = A little knowledge;\n",
    "            * 2 = A lot of knowledge.\n",
    "        * **behavioral_antiviral_meds** - Has taken antiviral medications.\n",
    "            * (binary)\n",
    "        * **behavioral_avoidance** - Has avoided close contact with other with flu-like symptoms.\n",
    "            * (binary)\n",
    "        * **behavioral_face_mask** - Has bought a face mask.\n",
    "            * (binary)\n",
    "        * **behavioral_wash_hands** - Has frequently washed hands or used hand sanitizer.\n",
    "            * (binary)\n",
    "        * **behavioral_large_gatherings** - Has reduced time at large gatherings.\n",
    "            * (binary)\n",
    "        * **behavioral_outside_home** - Has reduced contact with people outside of own household.\n",
    "            * (binary)\n",
    "        * **behavioral_touch_face** - Has avoided touching eyes, nose, or mouth.\n",
    "            * (binary)\n",
    "        * **doctor_recc_h1n1** - H1N1 flu vaccine was recommended by doctor.\n",
    "            * (binary)\n",
    "        * **doctor_recc_seasonal** - Seasonal flu vaccine was recommended by doctor.\n",
    "            * (binary)\n",
    "        * **chronic_med_condition** - Has any of the following chronic medical conditions: asthma or an other lung condition, diabetes, a heart condition, a kidney condition, sickle cell anemia or other anemia, a neurological or neuromuscular condition, a liver condition, or a weakened immune system caused by a chronic illness or by medicines taken for a chronic illness.\n",
    "            * (binary)\n",
    "        * **child_under_6_months** - Has regular close contact with a child under the age of six months.\n",
    "            * (binary)\n",
    "        * **health_worker** - Is a healthcare worker.\n",
    "            * (binary)\n",
    "        * **health_insurance** - Has health insurance.\n",
    "            * (binary)\n",
    "        * **opinion_h1n1_vacc_effective** - Respondent's opinion about H1N1 vaccine effectiveness.\n",
    "            * 1 = Not at all effective;\n",
    "            * 2 = Not very effective;\n",
    "            * 3 = Don't know;\n",
    "            * 4 = Somehwat effective;\n",
    "            * 5 = Very effective.\n",
    "        * **opinion_h1n1_risk** - Respondent's opinion about risk of getting sick with H1N1 flu without vaccine.\n",
    "            * 1 = Very low;\n",
    "            * 2 = Somewhat low;\n",
    "            * 3 = Don't know;\n",
    "            * 4 = Somewhat high;\n",
    "            * 5 = Very high.\n",
    "        * **opinion_h1n1_sick_from_vacc** - Respondent's worry about getting sick from taking H1N1 vaccine.\n",
    "            * 1 = Not at all worried;\n",
    "            * 2 = Not very worried;\n",
    "            * 3 = Don't know;\n",
    "            * 4 = Somewhat worried;\n",
    "            * 5 = Very worried.\n",
    "        * **opinion_seas_vacc_effective** - Respondent's opinion about seasonal flu vaccine effectiveness.\n",
    "            * 1 = Not at all effective;\n",
    "            * 2 = Not very effective;\n",
    "            * 3 = Don't know;\n",
    "            * 4 = Somehwat effective;\n",
    "            * 5 = Very effective.\n",
    "        * **opinion_seas_risk** - Respondent's opinion about risk of getting sick with seasonal flu without vaccine.\n",
    "            * 1 = Very low;\n",
    "            * 2 = Somewhat low;\n",
    "            * 3 = Don't know;\n",
    "            * 4 = Somewhat high;\n",
    "            * 5 = Very high.\n",
    "        * **opinion_seas_sick_from_vacc** - Respondent's worry about getting sick from taking seasonal flu vaccine.\n",
    "            * 1 = Not at all worried;\n",
    "            * 2 = Not very worried;\n",
    "            * 3 = Don't know;\n",
    "            * 4 = Somewhat worried;\n",
    "            * 5 = Very worried.\n",
    "        * **age_group** - Age group of respondent.\n",
    "        * **education** - Self-reported education level.\n",
    "        * **race** - Race of respondent.\n",
    "        * **sex** - Sex of respondent.\n",
    "        * **income_poverty** - Household annual income of respondent with respect to 2008 Census poverty thresholds.\n",
    "        * **marital_status** - Marital status of respondent.\n",
    "        * **rent_or_own** - Housing situation of respondent.\n",
    "        * **employment_status** - Employment status of respondent.\n",
    "        * **hhs_geo_region** - Respondent's residence using a 10-region geographic classification defined by the U.S. Dept. of Health and Human Services. Values are represented as short random character strings.\n",
    "        * **census_msa** - Respondent's residence within metropolitan statistical areas (MSA) as defined by the U.S. Census.\n",
    "        * **household_adults** - Number of *other* adults in household, top-coded to 3.\n",
    "        * **household_children** - Number of children in household, top-coded to 3.\n",
    "        * **employment_industry** - Type of industry respondent is employed in. Values are represented as short random character strings.\n",
    "        * **employment_occupation** - Type of occupation of respondent. Values are represented as short random character strings.\n",
    "* What is the target variable?\n",
    "    * There are two target variables in this dataset:\n",
    "        * **h1n1_vaccine** - Whether respondent received H1N1 flu vaccine.\n",
    "        * **seasonal_vaccine** - Whether respondent received seasonal flu vaccine.\n",
    "    * **This project will focus on the h1n1_vaccine target, specifically.**\n",
    "* What are the properties of the variables you intend to use?\n",
    "    * We intend to use all of the available variables, at least until any given variable is shown to be statistically insignificant.\n",
    "    * Most of the variables have numeric data types and will likely be left as-is. The variables which do not have numeric data types will likely be transformed using ordinal encoding or one hot encoding, as appropriate.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above descriptions of the features in this dataset were taken from DrivenData at the following URL:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.drivendata.org/competitions/66/flu-shot-learning/page/211/#features_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 10 HHS geographic classification regions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![regional-offices.png](images/regional-offices.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above image was taken from the U.S. Department of Health and Human Services at the following URL:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.hhs.gov/about/agencies/iea/regional-offices/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A brief word on MSA:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"The United States Office of Management and Budget (OMB) delineates metropolitan and micropolitan statistical areas according to published standards that are applied to Census Bureau data. The general concept of a metropolitan or micropolitan statistical area is that of a core area containing a substantial population nucleus, together with adjacent communities having a high degree of economic and social integration with that core. Currently delineated metropolitan and micropolitan statistical areas are based on application of 2010 standards (which appeared in the Federal Register on June 28, 2010) to 2010 Census and 2011-2015 American Community Survey data, as well as 2018 Population Estimates Program data. Current metropolitan and micropolitan statistical area delineations were announced by OMB effective March 2020.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above excerpt on MSA was taken from the United States Census Bureau at the following URL:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.census.gov/programs-surveys/metro-micro/about.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go ahead and import the standard packages we will use for this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import standard packages.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Packages are in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go ahead and read in the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the features of the dataset.\n",
    "features = pd.read_csv('./data/features.csv')\n",
    "\n",
    "# Quickly test the type of data in the dataframe.\n",
    "display(features.head())\n",
    "display(features.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the features dataframe has 36 columns, which corresponds correctly to our number of features, plus the respondent_id.<br>One thing worth noting here is that the dataframe indices and the respondent_id correspond perfectly, so we will likely set the index of the columns to respondent_id."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go ahead and read in the rest of the files and confirm whether or not they load in as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the labels of the dataset.\n",
    "labels = pd.read_csv('./data/labels.csv')\n",
    "\n",
    "# Quickly test the type of data in the dataframe.\n",
    "display(labels.head())\n",
    "display(labels.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the labels dataframe has 3 columns, which corresponds correctly to our number of labels, plus the respondent_id.<br>However, the respondent_id and the dataframe indices again correspond perfectly, so we will likely set the index of the columns to respondent_id.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that all of the files have been read in correctly, let's concatenate them into a dataframe so that we can execute our own train test splits on them and use them as a complete dataframe in later pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the dataframes.\n",
    "df = pd.concat([features, labels], axis=1)\n",
    "\n",
    "# Quickly test the type of data in the concatenated dataframe.\n",
    "display(df.head())\n",
    "display(df.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect. Now we have a single dataframe. However, our respondent_id column is in the dataframe twice now, so we will have to drop one of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fortunately, the dataframe index is actually the same as the respondent_id columns, so we can actually drop them both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop both of the respondent_id columns and save the changes to the df object.\n",
    "df.drop(columns='respondent_id', inplace=True)\n",
    "\n",
    "# Quickly test the type of data in the dataframe.\n",
    "display(df.head())\n",
    "display(df.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent. Now we have a complete dataframe with all of our features and our target, with no duplicate columns, and our index is set to the respondent_id."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start making any changes to the complete dataframe, let's make a copy of it in case something gets corrupted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign a copy dataframe variable and leave it alone for use in a backup situation.\n",
    "df_copy = df.copy()\n",
    "\n",
    "# Quickly test the type of data in the dataframe.\n",
    "display(df_copy.head())\n",
    "display(df_copy.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent. We have that dataframe set aside in case we mess up our data moving forward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a df object to use, let's conduct our preliminary data exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check out the df info to verify how many features we have, what they are,\n",
    "# how many nulls we have, and the data types of our features.\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we can see that we have the 35 expected features plus our targets, which we will split with our train test split.<br>The first thing that stands out here is that there are a significant number of nulls for the features 'health_insurance', 'employment_industry', and 'employment_occupation'. We need to take a look at these features to see how best to deal with those nulls and whether or not those features will be useable in our models. We will deal with these forms of data preparation shortly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore the shape of our dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's observe how many rows and columns are in our dataframe.\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on how shape compares to the indices of the dataframe, we can confidently say that every index is unique and the distribution of indices across the dataframe is uniform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore the size of our dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the number of rows by columns there are.\n",
    "df.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like there are just under 1 million individual intersections of row and column in this dataset. Hopefully this will give us enough datapoints to create some robust machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore the numerical description of our dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the descriptions into two displays to see all features.\n",
    "display(df.iloc[:, :12].describe())\n",
    "display(df.iloc[:, 12:].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The things that stick out here as being potentially noteworthy are these:\n",
    "* First, the mean of behavioral_antiviral_meds is incredibly low.\n",
    "* Second, the mean of behavioral_face_mask is incredibly low.\n",
    "* Third, the means and standard distributions of the opinion_h1n1_vacc_effective, opinion_h1n1_risk, and opinion_h1n1_sick_from_vacc are quite similar to the means and standard distributions of opinion_seas_vacc_effective, opinion_seas_risk, and opinion_seas_sick_from_vacc, respectively.\n",
    "* Fourth, despite having quite a high mean for opinion_h1n1_vacc_effective and moderate means for opinion_h1n1_risk and opinion_h1n1_sick_from_vacc, there is a fairly low mean for h1n1_vaccine.\n",
    "* Finally, despite the similarities between the h1n1 opinion features and the seasonal flu opinion features, the mean for seasonal_vaccine is much higher than the mean for h1n1_vaccine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at a random sample from our dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the values of a random sample's features, split so we can see them all.\n",
    "display(df.iloc[:, :18].sample(random_state=42))\n",
    "display(df.iloc[:, 18:].sample(random_state=42))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can say the following things about this particular respondent:\n",
    "* They were **somewhat concerned** about the H1N1 flu.\n",
    "* They had **a little knowledge** about H1N1 flu.\n",
    "* They had **not** taken antiviral medications.\n",
    "* They **had** avoided close contact with others with flu-like symptoms.\n",
    "* They **had** bought a face mask.\n",
    "* They **had** frequently washed their hands or use hand sanitizer.\n",
    "* They **had** reduced their time at large gatherings.\n",
    "* They **had** reduced their contact with people outside of their own household.\n",
    "* They **had** avoided touching their eyes, their nose, or their mouth.\n",
    "* The H1N1 flu vaccine was **not** recommended to them by a doctor.\n",
    "* The seasonal flu vaccine was **not** recommended to them by a doctor.\n",
    "\n",
    "This particular respondent did not answer many of the other questions, but we do also know the following:\n",
    "* They are between the ages of **18** and **34**.\n",
    "* They are **white**.\n",
    "* They are **female**.\n",
    "* They did **not** get the **H1N1** flu vaccine **or** the **seasonal** flu vaccine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing that may be interesting about the data itself is that, while the dataset as a whole does not have a large number of nulls (apart from the features we noticed earlier), this particular respondent had null entries for many features. This may indicate an overlap of null value counts between features in the entries for specific respondents, so we may be able to drop the remaining nulls after dealing with our large null count features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at how many nulls there are by feature in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First create a dataframe with true or false values for whether or not the associated cell is null,\n",
    "# then sum up all the times a cell in this dataframe was true.\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some interesting things to note here are these:\n",
    "* The number of null counts for doctor_recc_h1n1 and the number of null counts for doctor_recc_seasonal are exactly the same.\n",
    "* The number of null counts for the features health_insurance, employment_industry, and employment_occupation are all quite high, which is consistent with our findings from earlier.\n",
    "* Given a sample size of 26,707, the only feature other than those three with a null count higher than 10% of the sample size (2,670) is income_poverty, which has a null count of 4,423. Depending on how many respondents submitted null responses for multiple features, it may be possible to simply drop all of the nulls from the dataframe after dealing with the three large null count features from earlier, **with the exception of income_poverty**. We will either have to find a way to impute some values for the feature or drop it, as well, depending on the distribution of its value counts.\n",
    "* The only features with no null counts at all are age_group, race, sex, hhs_geo_region, census_msa, h1n1_vaccine, and seasonal_vaccine. You may recall that these were the only features our sample respondent answered, after docc_recc_seasonal. This may tell us that answers to these questions were mandatory, or that any survey response which was missing a value in any of these questions was not included in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore how many unique values there are in each feature in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the counts of unique values in each column in the dataset.\n",
    "df.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the above collection of unique values in each feature, we can see that all of our numeric features have the expected number of values, as laid out by the feature descriptions above. However, we don't yet know what the various value counts for the object type columns are, so let's take a look at those briefly. In addition to simply understanding what sorts of answers were available on the survey, this will help us determine what sort of transformation would be appropriate for each feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the value counts for the categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Return the value counts for every column in the dataframe that has a data type of object.\n",
    "for column in df.select_dtypes(include=object):\n",
    "    display(df[column].value_counts())\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have observed the value counts of income_poverty, we can see once again that imputing our 4,423 values into any one of these categories will have a significant impact on the distribution of the values of the feature. We cannot impute the mean because the feature is categorical and we cannot impute the median without fudging the data. With this in mind, we will likely drop this feature from the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now we have an idea of what values we can expect to see in each of our categorical features. None of these features have values with any sort of inherent rank, so it may be appropriate to use a one hot encoder on all of these features in our data preparation, as opposed to an ordinal encoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look and see how many respondents were duplicated in the dataset, if there are any at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First create a series with true or false values for whether or not\n",
    "# the associated row is identical to any other row,\n",
    "# then sum up all the times a value in this series was true.\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good deal. It looks like there were no respondents who submitted the exact same survey twice or, if there were, they have already been removed from the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's take a look at what sort of correlation exists between the numerical elements of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a correlation dataframe with the Pearson correlation coefficient\n",
    "# of the targets and every feature to the other targets and the other features.\n",
    "df.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is helpful for understanding how our features may be related, but let's go ahead and visualize it to make it easier to interpret."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a heat map of the correlation of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure so that the size of the image may be scaled up.\n",
    "fig, ax = plt.subplots(figsize=(16,12))\n",
    "\n",
    "# Create a heatmap and annotate the intersections with the associated correlation\n",
    "# for easier interpretation of the gradient.\n",
    "display(sns.heatmap(df.corr(), annot=True));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at this heatmap, we can see a few interesting things:\n",
    "* First, based on the gradient of the legend at the right, we can see that no feature or target negatively correlates to any other feature or target by a coefficient of -0.2 or lower, so we can say that no feature or target has a high negative correlation with any other feature or target.\n",
    "* Second, there are no features or targets that have a correlation with h1n1_vaccine above 0.5, so we can say that no feature or target has a strong relationship with h1n1_vaccine.\n",
    "* Third, there are three features with have a correlation with h1n1_vaccine greater than 0.3:\n",
    "    * doctor_recc_h1n1\n",
    "        * H1N1 flu vaccine was recommended by doctor. (binary)\n",
    "    * opinion_h1n1_risk\n",
    "        * Respondent's opinion about risk of getting sick with H1N1 flu without vaccine. (ordinal)\n",
    "    * seasonal_vaccine\n",
    "        * Whether respondent received seasonal flu vaccine. (binary)\n",
    "    * Based on these correlations, we can say that there may be a moderate relationship between these features and the target. However, in order to know whether or not this correlation is causative, we will need to explore these relationships in our modeling process. As we know, correlation does not necessarily mean causation; it just gives us some insight into what we may want to explore.\n",
    "* Finally, there are several feature which have correlations higher than 0.5:\n",
    "    * behavioral_large_gatherings and behavioral_outside_home\n",
    "    * doctor_recc_h1n1 and doctor_recc_seasonal\n",
    "    * opinion_h1n1_risk and opinion_seas_risk\n",
    "    * Based on this level of correlation, there may be a level of multicollinearity between these variables, but this is also something to explore during model iteration. We'll simply keep this in mind for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have completed all of our preliminary data exploration, it's time to begin preparing the data for modeling. We will be executing the changes we took note of earlier in the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "Desscribe and justify the process for preparing the data for analysis.\n",
    "\n",
    "***\n",
    "\n",
    "Questions to consider:\n",
    "\n",
    "* Were there variables you dropped or created?\n",
    "    * The variables we dropped were as follows:\n",
    "        * health_insurance\n",
    "        * employment_industry\n",
    "        * employment_occupation\n",
    "        * income_poverty\n",
    "    * In the case of all four of the above features, we elected to drop them because there were simply too many null values for the feature to be useable across the entire dataset without introducing artbitrarily imputed values.\n",
    "    * We created one hot encoded variables for all of the remaining object data type features.\n",
    "* How did you address missing values or outliers?\n",
    "    * We dropped categorical features with too many missing values.\n",
    "* Why are these choices appropriate given the data and the business problem?\n",
    "    * These choices are appropriate given the types of the data with missing values.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take some time to address the features we thought we might drop from our data exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the value counts of the health insurance feature to observe its distribution.\n",
    "df['health_insurance'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it looks like all of our values are either 1 or 0, again as expected. In terms of what we could do to rectify the null counts in this column, we could potentially impute the mean or the median, but both of these approaches will likely fudge the data. If we impute the mean, we change the distribution of the entire column from binary to something else, so we can't do that. If we impute the median, we drastically sway an already imbalanced distribution even more in favor of the majority, so we can't really do that, either. With respect to this column missing as many values as it does, the best approach for this feature may simply be to drop it altogether."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's drop it now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the health insurance feature from the dataset,\n",
    "# as it is missing too many values to use.\n",
    "df.drop(columns='health_insurance', inplace=True)\n",
    "\n",
    "# Quickly test the type of data in the concatenated dataframe.\n",
    "display(df.head())\n",
    "display(df.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see by the number of columns that our number of features has been reduced by one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the same reasons we mentioned above with respect to health_insurance, we will also be dropping employment_industry and employment_occupation. Let's do that now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the employment industry and employment occupation features from the dataset,\n",
    "# as they are missing too many values to use.\n",
    "df.drop(columns=['employment_industry', 'employment_occupation'], inplace=True)\n",
    "\n",
    "# Quickly test the type of data in the concatenated dataframe.\n",
    "display(df.head())\n",
    "display(df.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see by the number of columns that our number of features has been reduced by two."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we can see that the null count of income_poverty is also somewhat high, let's take a look at the value counts distribution of that feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the various values for income_poverty\n",
    "# and how times each value occurs in the dataframe.\n",
    "df['income_poverty'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, unfortunately, we may be in a similar pickle to the one we were in with the big three, as imputing 4,423 entries for any of these values will have a significant impact on the distribution of this feature. We'll go ahead and drop this feature from our modeling process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's drop it now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the income poverty feature from the dataset,\n",
    "# as it is missing more than 10% of the values in the feature.\n",
    "df.drop(columns='income_poverty', inplace=True)\n",
    "\n",
    "# Quickly test the type of data in the concatenated dataframe.\n",
    "display(df.head())\n",
    "display(df.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, we can see by the number of columns that our number of features has been reduced by one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have dropped the features we plan on dropping, let's take another look at the null counts of the dataframe and see whether or not we can simply drop the remaining nulls without reducing the number of entries by more than 10% of our current total number of entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check and see how many rows there are in the dataframe\n",
    "# before dropping all the nulls.\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we have 26,707 entries altogether, which is exactly what we would expect without having dropped any rows yet. This is consistent with what we learned in our data exploration. In order to be comfortable with simply dropping all of the nulls, we would want our number of entries after the drop to be no less than 26,707 - 2,671, or 24,106."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the null counts after the drops we have already made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check and confirm that no one null sum is over 2671.\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good deal. No one feature has more than 2,671 nulls in it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see whether or not we retain enough of our data if we simply drop all of the nulls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop every row in the dataframe that has a null value in any feature.\n",
    "df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, we can see that our number of rows would be 21,710 if we did this, which is less than the amount we would need to retain (24,106). Knowing this, we will have to make some decisions about how best to impute data for these features. Again, given that none of these features have null counts greater than 10% of their total counts, no one method of imputing will change the distribution of the values of a feature too much, but it still behooves us to think about the *best* way to impute these values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look once again at how many null counts there are of each feature, **compared to the value counts** of each feature. It is worth noting that, at this point, we will not actually be able to perform any imputations without introducing data leakage into our project. This will, however, help us prepare our data once we do a train test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for column in df:\n",
    "    print(\"-\"*50)\n",
    "    print(f\"\\n{column} value counts:\\n{(df[column].value_counts())}\\n\")\n",
    "    print(f\"\\n{column} null counts:\\n{df[column].isnull().sum()}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since every feature in this dataset is categorical, whether it is of a numeric data type or an object data type, the simplest and most appropriate path forward may be to simply impute the most frequent value for each of the nulls. This method of imputation will have the lowest impact on the distribution of the values of each feature, as the most frequent value already has the highest number of value counts. An imputation of the most frequent value will increase its value count by a smaller proportion than an imputation of any other value would."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go ahead and execute a train test split on our dataframe so we can impute some values and encode some features. We'll do this so we can manually create some simple models for now, but we'll employ pipelines and gridsearches later on to iterate through a large number of models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns='h1n1_vaccine')\n",
    "y = df['h1n1_vaccine']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "names_count = 0\n",
    "\n",
    "for sub_df in (X, y, X_train, y_train, X_test, y_test):\n",
    "    names = ('X', 'y', 'X_train', 'y_train', 'X_test', 'y_test')\n",
    "    print(\"-\"*100)\n",
    "    print(f\"\\n{names[names_count]} head:\\n\")\n",
    "    display(sub_df.head())\n",
    "    print(f\"\\n{names[names_count]} tail:\\n\")\n",
    "    display(sub_df.tail())\n",
    "    print(\"\\n\")\n",
    "    names_count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice. After our train test split, we can see that all of the indices of our subset dataframes line up. Now that we have our training data isolated from our test data, we can execute the following measures on our two feature dataframes:\n",
    "* 1 - A SimpleImputer, to eliminate nulls in the features we did not drop\n",
    "* 2 - A OneHotEncoder, to turn our object data type features into numerical data types\n",
    "* 3 - A StandardScaler, to ensure that the weight of each feature is being compared on the same scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's fit a simple imputer to our training data, transform the training data with it, and transform the test data with it, as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate an imputer variable.\n",
    "# We are using the most_frequent method here because it will work on object data types.\n",
    "si = SimpleImputer(strategy='most_frequent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate an imputer variable that is specifically fit to our training data.\n",
    "imputer = si.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate an imputed data set variable that is assigned to the transformation\n",
    "# of our Simple Imputer on our training data.\n",
    "X_train_imputed = imputer.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Double-check the data type of the new object.\n",
    "type(X_train_imputed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the new object to a dataframe.\n",
    "X_train_imputed = pd.DataFrame(X_train_imputed, columns=X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observe the head of the new dataframe.\n",
    "X_train_imputed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that our operation has removed the nulls from the dataframe.\n",
    "X_train_imputed.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the above transformation process to the test data.\n",
    "X_test_imputed = pd.DataFrame(imputer.transform(X_test), columns=X_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check it out.\n",
    "X_test_imputed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm the absence of nulls.\n",
    "X_test_imputed.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looks like this operation made all of our features objects.\n",
    "# We will convert the numeric ones back momentarily.\n",
    "X_train_imputed.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_imputed.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like our SimpleImputer was a success. We now have a dataframe with no nulls at all in it. However, all of our data types are now objects, so we will have to convert them back to numbers at the appropriate time. Additionally, there is still the matter of encoding the features that were actually of the object data type to begin with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's separate the number features and the object features so we can perform the appropriate transformations on the appropriate values. Once we've done that, we'll encode the objects and combine everything back into one final dataframe for our modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a list of all the features we need to encode with our OneHotEncoder.\n",
    "features_to_encode = list(df.select_dtypes(include=object))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm that these features are the ones that we want.\n",
    "features_to_encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe that consists only of the features which were originally numbers,\n",
    "# so we can convert them back to numbers after we did our imputing.\n",
    "X_train_numbers = X_train_imputed.drop(columns=features_to_encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm that these are the features that we want.\n",
    "X_train_numbers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_numbers.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_numbers.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recast all of these features as floats.\n",
    "X_train_numbers = X_train_numbers.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm that the operation was a success.\n",
    "X_train_numbers.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply a filter of our features to encode on the X_train_imputed dataframe,\n",
    "# so we can encode only those features of that dataframe.\n",
    "X_train_objects = X_train_imputed[features_to_encode]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm that these are the features we want.\n",
    "X_train_objects.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_objects.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate an encoder variable.\n",
    "# We will be using the ignore argument for the handle_unknown paramter,\n",
    "# because we want our encoder to be able to handle values it hasn't seen\n",
    "# once it has been trained. Sparse is set to false so this encoder\n",
    "# will return an array instead of a sparse matrix.\n",
    "ohe = OneHotEncoder(handle_unknown='ignore', sparse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit our encoder onto the objects features of our imputed training data.\n",
    "object_encoder = ohe.fit(X_train_objects, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform those features with the fit we just trained the encoder on.\n",
    "X_train_objects_encoded = object_encoder.transform(X_train_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observe the data type of the new object.\n",
    "type(X_train_objects_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As before, convert the object into a dataframe.\n",
    "X_train_objects_encoded = pd.DataFrame(X_train_objects_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our column names of the encoded features don't confer any value to the features,\n",
    "# so we will need to rename the columns in a way that is useful.\n",
    "X_train_objects_encoded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create feature names based off the original feature name\n",
    "# and the value present in the new encoded column.\n",
    "X_train_objects_encoded.columns = ohe.get_feature_names(X_train_objects.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm that our operation was a success.\n",
    "X_train_objects_encoded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that all of our number features are in a dataframe with no nulls or objects and our object features are in a dataframe with no nulls, no numbers, and encoded feature columns, let's put them back together into our final dataframe so we can scale it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's concatenate the dataframes now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the dataframes about axis number 1, which is columns.\n",
    "X_train_transformed = pd.concat([X_train_numbers, X_train_objects_encoded], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm that our operation was a success.\n",
    "X_train_transformed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observe the state of the final dataframe before scaling.\n",
    "X_train_transformed.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very good. Now we need to perform the same operations to our test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll separate the feature types again and make the appropriate transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_numbers = (X_test_imputed.drop(columns=features_to_encode)).astype(float)\n",
    "X_test_objects = X_test_imputed[features_to_encode]\n",
    "X_test_objects_encoded = pd.DataFrame(object_encoder.transform(X_test_objects))\n",
    "X_test_objects_encoded.columns = ohe.get_feature_names(X_test_objects.columns)\n",
    "X_test_transformed = pd.concat([X_test_numbers, X_test_objects_encoded], axis=1)\n",
    "X_test_transformed.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent. Both our training features and test features are in the shape that we need them to be. The final transformation is a scaler, so the weights of the features can be properly assessed by the models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use a StandardScaler here because it accounts fairly well for means and standard deviations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = StandardScaler()\n",
    "feature_scaler = ss.fit(X_train_transformed)\n",
    "X_train_scaled = feature_scaler.transform(X_train_transformed)\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train_transformed.columns)\n",
    "X_test_scaled = feature_scaler.transform(X_test_transformed)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test_transformed.columns)\n",
    "\n",
    "display(X_train_scaled.head())\n",
    "display(X_test_scaled.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And there you have it! We are finally ready for modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Modeling\n",
    "\n",
    "Describe and justify the process for analyzing or modeling the data.\n",
    "\n",
    "***\n",
    "\n",
    "Questions to consider:\n",
    "\n",
    "* How did you analyze or model the data?\n",
    "* How did you iterate on your initial approach to make it better?\n",
    "* Why are these choices appropriate given the data and the business problem?\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first model we will look at is a dummy model, to establish a baseline understanding of how good any other model would have to be in order to perform better than the worst model we could make."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = DummyClassifier(strategy='most_frequent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_dummy = dummy.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_dummy_preds = fit_dummy.predict(X_train_scaled)\n",
    "print(f\"X train dummy model predictions:\\n{X_train_dummy_preds}\")\n",
    "X_test_dummy_preds = fit_dummy.predict(X_test_scaled)\n",
    "print(f\"X test dummy model predictions:\\n{X_test_dummy_preds}\")\n",
    "X_train_dummy_score = round(fit_dummy.score(X_train_scaled, y_train), 6)\n",
    "print(f\"X train dummy score:\\n{X_train_dummy_score}\")\n",
    "X_test_dummy_score = round(fit_dummy.score(X_test_scaled, y_test), 6)\n",
    "print(f\"X test dummy score:\\n{X_test_dummy_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we can say based off of this dummy model evaluation is that it is accurately predicting the class of the training target approximately 78.74687% of the time, and that it is accurately predicting the class of the testing target approximately 78.77789% of the time. The reason this model is as accurate as it is is because of the distribution of the values of the target. If we execute a normalized value counts for the target class, we should see that the highest normalized value count is equal to our score, or very close to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rounded to the default number of places, we can see that these values are identical. Let's move on to our first simple model, a LogisticRegression, to see how well it performs with the default values. Then, we will manually tune the hyperparameters to see if we can improve its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_one = LogisticRegression(random_state=42, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_one_fit = logreg_one.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_one_train_preds = logreg_one_fit.predict(X_train_scaled)\n",
    "logreg_one_test_preds = logreg_one_fit.predict(X_test_scaled)\n",
    "logreg_one_train_score = round(logreg_one_fit.score(X_train_scaled, y_train), 6)\n",
    "logreg_one_test_score = round(logreg_one_fit.score(X_test_scaled, y_test), 6)\n",
    "print(f\"lr_1_train_score: {logreg_one_train_score}\")\n",
    "print(f\"lr_1_test_score: {logreg_one_test_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round((logreg_one_test_score - X_test_dummy_score), 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right off the bat, we can see an improvement in the accuracy of this model. Compared to our dummy test score of 0.787779, our first logistic regression has an accuracy of 0.850681. This is an increase of 0.062902, or 6.2902%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if we can improve the performance of this model by changing the arguments for various hyperparameters via grid searching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_one_param_grid_one = {\n",
    "    'tol': (0.00001, 0.0001, 0.001),\n",
    "    'C': (0.4, 0.5, 0.6),\n",
    "    'max_iter': (10, 100, 1000)\n",
    "}\n",
    "\n",
    "logreg_one_gs_one = GridSearchCV(logreg_one, logreg_one_param_grid_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "logreg_one_gs_one.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_one_gs_one.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_one_gs_one.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_one_gs_one.score(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, it looks like our best performing model in this grid search was one with a C value of 0.4, a max_iter value of 100, and a tol value of 1e-05. This score is slightly than our original score, and we have also gained the benefit of cross-validation through grid searching."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that our best C score was on the low end of our range for that hyperparameter and that our best tol value was also on the low end of our range for that hyperparameter, it is worth reiterating the grid search with extended ranges for those hyperparamters. Let's do that now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_one_param_grid_two = {\n",
    "    'tol': (0.000001, 0.00001, 0.0001, 0.001),\n",
    "    'C': (0.3, 0.4, 0.5, 0.6),\n",
    "    'max_iter': (10, 100, 1000)\n",
    "}\n",
    "\n",
    "logreg_one_gs_two = GridSearchCV(logreg_one, logreg_one_param_grid_two)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_one_gs_two.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_one_gs_two.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_one_gs_two.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_one_gs_two.score(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on how well this grid search performed on the test data, it looks like our previous best model was actually doing a better job of accurately predicting the target class than this grid search. All of our best test scores so far, however, have been quite close, so continuing to tune the hyperparameters of this model may not yield significant gains in performance.<br><br>Let's move on to another model for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've tried a simple model, let's try something more complex, like a random forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_one = RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Evaluation\n",
    "\n",
    "Evaluate how well your work solves the stated business problem.\n",
    "\n",
    "***\n",
    "\n",
    "Questions to consider:\n",
    "\n",
    "* How do you interpret the results?\n",
    "* How well does your model fir your data?\n",
    "* How much better is this than your baseline model?\n",
    "* How confident are you that your results would generalize beyond the data you have?\n",
    "* How confident are you that this model would benefit the business if put into use?\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "Provide your conclusions about the work you've done, including any limitations or next steps.\n",
    "\n",
    "***\n",
    "\n",
    "Questions to consider:\n",
    "\n",
    "* What would you recommend the business do as a result of this work?\n",
    "* What are some reasons why your analysis might not fully solve the business problem?\n",
    "* What else could you do  in the future to improve this project?\n",
    "\n",
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
